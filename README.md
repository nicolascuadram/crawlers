# crawlers
Sistema Unificado de Extracci√≥n de Informaci√≥n para el Curso de Gesti√≥n de Proyectos Tecnol√≥gicos.

# üìÑ Documentaci√≥n t√©cnica: Agregar una nueva fuente de scraping

Este sistema permite la integraci√≥n de nuevas fuentes de art√≠culos acad√©micos o t√©cnicos mediante scrapers personalizados. La arquitectura est√° dise√±ada para ser modular y extensible.

## üß± Estructura del sistema

    backend/scraping/: Carpeta que contiene los scrapers individuales por fuente.

    backend/crawler.py : Archivo principal que coordina la ejecuci√≥n de scrapers.

    Base de datos PostgreSQL con las tablas: source, post, log.

## ‚úÖ Requisitos para agregar una nueva fuente
### Paso 1: Agregar la fuente a la base de datos

Ejecuta un INSERT INTO en la tabla source para registrar la nueva fuente:

INSERT INTO source (id, name, url, type, active, created_at) 
VALUES (6, 'Nombre de la nueva fuente', 'https://url-de-la-fuente.com', 'article', true, CURRENT_TIMESTAMP);

    - id: Valor √∫nico.

    - name: Nombre que usar√°s tambi√©n para el routing del scraper.

    - url: URL de origen para scrapear.

    - type: Tipo de contenido de la fuente, por ejemplo, articulos. 

    - active: true para que se ejecute.

    - created_at: CURRENT_TIMESTAMP.

### Paso 2: Crear el scraper

Dentro del directorio backend/scraping/, crea un nuevo archivo, por ejemplo:

`scraping/nueva_fuente.py`

Ejemplo b√°sico de scraper (retorna lista de post):

```python
import request
from bs4 import BeautifulSoup

def scrape_nueva_fuente(base_url):
    posts = []

    response = requests.get(base_url)
    soup = BeautifulSoup(response.content, 'html.parser')

    for item in soup.select(".post"):  # Ajustar al selector real
        post = {
            'title': item.select_one(".title").get_text(strip=True),
            'summary': item.select_one(".keywords").get_text(strip=True),
            'content': item.select_one(".abstract").get_text(strip=True),
            'url': item.select_one(".doi a")["href"],
            'type': 'article',
            'published_at': item.select_one(".date").get_text(strip=True),
            'sourcename': 'Nombre de la nueva fuente',
            'downloadArticleLink': item.select_one(".pdf a")["href"],
            'authors': ", ".join([a.get_text(strip=True) for a in item.select(".authors span")])
        }
        posts.append(post)

    return posts
```

La l√≥gica o librerias a usar en el scraping pueden variar para cada fuente, pero es necesario que la funci√≥n usada para scrapear, reciba como argumento solo la url objetivo y retorne un arreglo de objetos con la siguiente estructura:
```python
post = {
            'title': string, # titulo de los articulos
            'summary': string, # palabras claves de los articulos
            'content': string, # abstract del articulo
            'url': string, # link a la pagina del articulo DOI
            'type': string, # article, paper, etc
            'published_at': string, # fecha de publicacion
            'sourcename': string, # Nombre de la fuente de los posts
            'downloadArticleLink': string, # link para descargar el pdf o de no haberlo el doi nuevamente
            'authors': string # lista de autores del articulo
        }

```

### Paso 3: Importar el scraper

Ve al archivo principal de scraping (crawler.py) y agrega la importaci√≥n:

`from scraping.nueva_fuente import scrape_nueva_fuente`

### Paso 4: Agregarlo al router de scrapers

Dentro de la funci√≥n `scrape_source`, a√±ade el if correspondiente usando el nombre registrado en la base de datos:
```python
if source_name.lower() == 'nombre de la nueva fuente':
    return scrape_nueva_fuente(source_url)
```

Aseg√∫rate de que el texto de comparaci√≥n (source_name.lower()) coincida exactamente con el campo name en la base de datos en min√∫sculas.

#### üì¶ Formato esperado de cada post

Cada scraper debe devolver una lista de diccionarios con la siguiente estructura:

```python
post = {
    'title': string,               # T√≠tulo del art√≠culo
    'summary': string,             # Palabras clave
    'content': string,             # Resumen / Abstract
    'url': string,                 # URL del art√≠culo (preferentemente DOI)
    'type': string,                # Tipo de publicaci√≥n (ej. "article")
    'published_at': string,        # Fecha de publicaci√≥n (ISO 8601 preferido)
    'sourcename': string,          # Nombre de la fuente (debe coincidir con source.name)
    'downloadArticleLink': string,# Link de descarga (PDF o DOI)
    'authors': string              # Autores, separados por coma
}
```

üß™ Recomendaciones
    - Valida la existencia de campos antes de extraerlos para evitar errores (NoneType).
    - Loguea errores si una p√°gina no contiene lo esperado.
    - Testea manualmente el scraper antes de habilitarlo en producci√≥n.
    - Usa try/except dentro del scraper si es una fuente inconsistente.

üßπ Opcional: desactivar temporalmente una fuente

`UPDATE source SET active = false WHERE name = 'Nombre de la fuente';`
